{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Pokemon_Autoencoder_tf.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Y5_DRdLdYZFJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"51f182ff-74a4-45f2-fede-0bc78fa22cc8","executionInfo":{"status":"ok","timestamp":1586192863190,"user_tz":-480,"elapsed":22041,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W-bf3LC8BQMe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b4d9008e-bb69-4cbe-c06f-48e4c4ee1252","executionInfo":{"status":"ok","timestamp":1586192863193,"user_tz":-480,"elapsed":22024,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from IPython import display\n","print(matplotlib.get_backend())\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["module://ipykernel.pylab.backend_inline\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3VdH75A6BQMp","colab_type":"text"},"source":["#  寶可夢自動編碼器 (tensorflow)"]},{"cell_type":"markdown","metadata":{"id":"pkCFXUE0Yi_V","colab_type":"text"},"source":["支援python 版本: 3.5以上  \n","支援pytorch版本 : 1.2以上"]},{"cell_type":"markdown","metadata":{"id":"n8rEbsdvYoss","colab_type":"text"},"source":["深度學習的關鍵就是「表徵學習( representation learning)」，透過最佳化算法算更新神經元權重的同時，也正是將所搜尋到的特徵進一步檢視是有用的保留，還是冗餘的捨棄，而  autoencoder  自動編碼器正是找尋關鍵特徵並將其充分壓縮的經典網路結構。在這個實作範例中，我們將帶著大家設計一個簡單的卷積自編碼器，而輸入的數據正是目前很流行的寶可夢，我們要來實證看看，光是利用沒有做任何標註的數據，自編碼器是否能夠有效的找出關鍵特徵。"]},{"cell_type":"code","metadata":{"id":"bMilNPk6BQMu","colab_type":"code","outputId":"5582ec0b-4eda-456b-e7d6-91d4ff3eb8e3","colab":{"base_uri":"https://localhost:8080/","height":607},"executionInfo":{"status":"ok","timestamp":1586192886688,"user_tz":-480,"elapsed":45488,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["import glob\n","import os\n","import cv2\n","os.environ['TRIDENT_BACKEND'] = 'pytorch'\n","os.environ['TRIDENT_HOME'] = '/content/gdrive/My Drive/trident'\n","!pip uninstall tridentx\n","!pip install '/content/gdrive/My Drive/DeepBelief_Course5_Examples/tridentx-0.5.0-py3-none-any.whl'\n","#!pip install tridentx --upgrade\n","import trident as T\n","from trident import *"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[33mWARNING: Skipping tridentx as it is not installed.\u001b[0m\n","Processing ./gdrive/My Drive/DeepBelief_Course5_Examples/tridentx-0.5.0-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (1.18.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (4.38.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1scipy>=1.2 in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (7.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (2.21.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (2.10.0)\n","Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.6/dist-packages (from tridentx==0.5.0) (0.16.2)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tridentx==0.5.0) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tridentx==0.5.0) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tridentx==0.5.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tridentx==0.5.0) (2019.11.28)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->tridentx==0.5.0) (1.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->tridentx==0.5.0) (2.4.1)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->tridentx==0.5.0) (1.4.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->tridentx==0.5.0) (3.2.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->tridentx==0.5.0) (2.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->tridentx==0.5.0) (2.4.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->tridentx==0.5.0) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->tridentx==0.5.0) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->tridentx==0.5.0) (0.10.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14->tridentx==0.5.0) (4.4.2)\n","Installing collected packages: tridentx\n","Successfully installed tridentx-0.5.0\n"],"name":"stdout"},{"output_type":"stream","text":["trident 0.5.0\n","Using Pytorch backend.\n","Image Data Format: channels_first.\n","Image Channel Order: rgb.\n","Using pillow image backend.\n","Pillow version:7.0.0.\n","Pytorch version:1.4.0.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"eMbY5U8bYzMc","colab_type":"text"},"source":["首先載入這次這個實作所需要的工具包，各位可以發現我沒有匯入pytorch相關的包，而改匯入trident ， trident 是我為了便利教學以及簡化開發流程所設計的新的 api ，開發它的原因在於不管是不熟悉 python的初學者或是精通深度學習的開發者。都會因為建立分析過程中各種繁瑣的細節、框架的差異以及常疏漏的設定等而陷入在痛苦的填坑之路， trident 希望學習者或開發者可以不用再重複造輪子以及希望能降低大家掉到坑裡的機率。各位可以直接利用 pip install tridentx來安裝(如果是在 jupyter notebook上執行安裝請記得前方加 !)\n","\n","trident能夠如何簡化分析流程呢?以下一句語法為例，我們只需要一行語法就能直接下載並且讀取我們課程的數據集( 除了上課範例數據集外， trident也內建了不少經典數據集)，各位可能覺得那有甚麼稀奇， keras以及 torchvision早就有了這功能， trident跟他們不同的地方，若是您宣告了\n","\n","os.environ['TRIDENT_BACKEND'] = 'pytorch'\n","\n","那麼， trident 就會自動地把影像數據格式轉成以 CHW, BGR, 目標標籤不做 onehot ，若是 tensorflow自動轉成 HWC, RGB, 目標標籤做 onehot ，若是 cntk 自動轉成 CHW, BGR, 目標標籤做 onehot ，若是使用  opencv 則會自動將 BGR轉 RGB，覺得有點意思了吧。 trident 這個字的原意是三叉戟，代表著這三個深度學習框架，我並非要像 keras一樣設計一個高階 api企圖一統各種框架，但實際上卻因為遷就框架之間差異太大失去了原有的簡潔性，相反的我就是理解以及知道框架之間的差異過大，因此 trident 比較像是一個跨框架的平行開發範本，我們盡量在不同框架中能讓各位有一致的開發體驗，也就是學一次就能輕易地跨框架使用。"]},{"cell_type":"code","metadata":{"id":"1XDOQc55BQM2","colab_type":"code","outputId":"bb5fc4fb-ac75-4144-bf74-ba95a81acc14","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1586192903944,"user_tz":-480,"elapsed":62716,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["dataset=T.load_examples_data('pokemon')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["archive file is already existing, donnot need download again.\n","extraction is finished, donnot need extract again.\n","/content/gdrive/My Drive/trident/datasets/examples_pokemon/pokemon\n","get pokemon images :1444\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mkbgPsJdY6lY","colab_type":"text"},"source":["覺得torchvision 的 transform 開發很不方便嗎，在 trident 所有的轉換都是基於函數，輸入是圖檔(看影像後台是用什麼，預設是pillow)，輸出則是處理過的影像圖檔，至於執行的順序只需要用清單依序放置，指派給資料集的image_transform_funcs就可以了 。例如下面的語法就是先將影像大小縮放至 128*128 ，在正規化(減 127.5 除以127.5)，這樣數據在吐出來之前就會依照這些轉換依序完成。"]},{"cell_type":"code","metadata":{"id":"XBshKhgLBQM-","colab_type":"code","colab":{}},"source":["dataset.image_transform_funcs=[resize((130,130)),\n","                               random_crop(128,128),\n","                               normalize(127.5,127.5)]       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdyMzuIjZABd","colab_type":"text"},"source":["下圖是標準的自動編碼器的結構示意圖，它透過前半段尺寸越來越小的編碼器，將原始圖片壓縮編碼成特徵向量(在本次實作中是長度為 128的向量)後，在透過後半段尺寸逐漸增大的解碼器進行結構上的復原，而損失函數一般使用 MSELoss 也就是輸入圖與輸出圖差異的平方，目的就是希望神經網路夠找到一個最佳的編碼方式，讓極度壓縮過後的訊息能夠重建回原狀。"]},{"cell_type":"markdown","metadata":{"id":"7_n8nxUiZFFM","colab_type":"text"},"source":["<img src='https://docs.google.com/uc?export=download&id=1-brTOHQ7oh8Y24u-CQoYn1uq4VE9P_Fj' width='600px'/>"]},{"cell_type":"markdown","metadata":{"id":"W41oGnRpZKiH","colab_type":"text"},"source":["以下是我們分別設計模型的編碼器與解碼器部分，模型設計也是 trident比較特殊之處，因為在原本的 torch 中所有的卷積層都需要指定輸入的通道數( input-channel) ，同時也需要自己指定 padding 需要的量，同時一般建模所常用的卷積 ->正規化 ->活化函數都需要自己一層一層指定或者是自己把它包成自訂模組，在 trident 中不需要層層指定輸入通道數，只需要第一層指定輸入形狀、或是第一次 forward自行偵測輸入形狀、也可以透過shape_infer([input_size]) 就可以自動更新全體的input_filters ，此外padding 也可以替換為 auto_pad 即可自動計算保持原形狀所需的 padding量，同時內建的Conv2d_Block直接讓卷積、正規化、活化函數、 dropout 和添加噪音等功能都集合在一起，這樣開發起來就更方便了。"]},{"cell_type":"code","metadata":{"id":"dyjhmMloBQNM","colab_type":"code","colab":{}},"source":["\n","encoder=Sequential(\n","    Conv2d_Block((5,5),32,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False, add_noise=True,noise_intensity=0.05),#(32,128,128)\n","    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,64,64)\n","    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,32,32)\n","    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.5),#(128,16,16)\n","    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n","    Conv2d_Block((3,3),256,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(256,4,4)\n","    Conv2d_Block((3,3),256,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(256,4,4)\n","    Reshape((-1,1,1)), #(256*4*4)\n","    Conv2d((1,1),128,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n",")\n","\n","\n","decoder=Sequential(\n","    Conv2d((1,1),128*4*4,strides=1,auto_pad=True,activation='tanh',use_bias=False), #(2048,1,1 )\n","    Reshape((128,4,4)), #(128,4,4))\n","    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n","    TransConv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n","    TransConv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,16,16)\n","    TransConv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,32,32)\n","    TransConv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,64,64)\n","    TransConv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,128,128)\n","    Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n",")\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVENW-_-ZUA4","colab_type":"text"},"source":["自動編碼器的模型就只要將編碼器與解碼器連在一起就完成了"]},{"cell_type":"code","metadata":{"id":"QaULpSrJBQNU","colab_type":"code","colab":{}},"source":["autoencoder=Sequential(\n","    encoder,\n","    decoder\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ot8s-fYxZXbH","colab_type":"text"},"source":["剛才提到了有關於輸入形狀的推斷， trident 中的所有神經網路層都是支援延遲推斷的，這樣的好處是在於不需要逐層設定輸入通道數，只需要做一次(整體模型，也就是最外層)即可\n","\n"]},{"cell_type":"code","metadata":{"id":"RMbJ6WDfBQNc","colab_type":"code","outputId":"1f4f30a0-7e67-4b7b-b89f-3635575a9d18","colab":{"base_uri":"https://localhost:8080/","height":863},"executionInfo":{"status":"ok","timestamp":1586193222765,"user_tz":-480,"elapsed":1550,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["autoencoder.input_shape=[3,128,128]\n","summary(autoencoder,(3,128,128))\n","#如果你不想重頭跑，就請把下一行的註釋取消\n","#autoencoder=T.load('Models/pokemon_ae.pth')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------------------------------------------------------\n","              Layer (type)                   Output Shape            Weight           Bias    Param #     FLOPS #   \n","==============================================================================\n","conv2d_28                                [-1, 32, 128, 128]    [32, 3, 5, 5]                  2400      78626816.0  \n","batch_norm_14                            [-1, 32, 128, 128]    [32]                 [32]      64        1556480.0   \n","conv2d__block_9                          [-1, 32, 128, 128]                                   0         0.0         \n","batch_norm_15                            [-1, 64, 64, 64]      [64]                 [64]      128       782336.0    \n","conv2d__block_10                         [-1, 64, 64, 64]                                     0         0.0         \n","batch_norm_16                            [-1, 64, 32, 32]      [64]                 [64]      128       195584.0    \n","conv2d__block_11                         [-1, 64, 32, 32]                                     0         0.0         \n","batch_norm_17                            [-1, 128, 16, 16]     [128]                [128]     256       98048.0     \n","conv2d__block_12                         [-1, 128, 16, 16]                                    0         0.0         \n","batch_norm_18                            [-1, 128, 8, 8]       [128]                [128]     256       24512.0     \n","conv2d__block_13                         [-1, 128, 8, 8]                                      0         0.0         \n","batch_norm_19                            [-1, 256, 4, 4]       [256]                [256]     512       12272.0     \n","conv2d__block_14                         [-1, 256, 4, 4]                                      0         0.0         \n","batch_norm_20                            [-1, 256, 4, 4]       [256]                [256]     512       12272.0     \n","conv2d__block_15                         [-1, 256, 4, 4]                                      0         0.0         \n","reshape_3                                [-1, 4096, 1, 1]                                     0         0.0         \n","conv2d_25                                [-1, 128, 1, 1]       [128, 4096, 1, 1]              524288    1048575.0   \n","sequential_7                             [-1, 128, 1, 1]                                      0         0.0         \n","conv2d_26                                [-1, 2048, 1, 1]      [2048, 128, 1, 1]              262144    524287.0    \n","reshape_4                                [-1, 128, 4, 4]                                      0         0.0         \n","batch_norm_21                            [-1, 128, 4, 4]       [128]                [128]     256       6128.0      \n","conv2d__block_16                         [-1, 128, 4, 4]                                      0         0.0         \n","batch_norm_22                            [-1, 128, 8, 8]       [128]                [128]     256       24512.0     \n","trans_conv2d__block_6                    [-1, 128, 8, 8]                                      0         0.0         \n","batch_norm_23                            [-1, 64, 16, 16]      [64]                 [64]      128       48896.0     \n","trans_conv2d__block_7                    [-1, 64, 16, 16]                                     0         0.0         \n","batch_norm_24                            [-1, 64, 32, 32]      [64]                 [64]      128       195584.0    \n","trans_conv2d__block_8                    [-1, 64, 32, 32]                                     0         0.0         \n","batch_norm_25                            [-1, 64, 64, 64]      [64]                 [64]      128       782336.0    \n","trans_conv2d__block_9                    [-1, 64, 64, 64]                                     0         0.0         \n","batch_norm_26                            [-1, 64, 128, 128]    [64]                 [64]      128       3129344.0   \n","trans_conv2d__block_10                   [-1, 64, 128, 128]                                   0         0.0         \n","conv2d_27                                [-1, 3, 128, 128]     [3, 64, 1, 1]                  192       6275072.0   \n","sequential_8                             [-1, 3, 128, 128]                                    0         0.0         \n","================================================================\n","Total params: 791,904\n","Trainable params: 791,904\n","Non-trainable params: 0\n","Total MACC: 45,557,760.0\n","Total FLOPs: 0.09334 GFLOPs\n","----------------------------------------------------------------\n","Input size (MB): 0.19\n","Forward/backward pass size (MB): 39.97\n","Params size (MB): 3.02\n","Estimated Total Size (MB): 43.18\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W4nzWLnrZeY_","colab_type":"text"},"source":["傳統的pytorch訓練時非常繁瑣，像是要切換訓練模式model.train()、清掉梯度zero_grad()、反向傳播 loss.backward() 、進行下個批次 optimizer.step() ，這些已經夠煩人了，更別提向量進入模型前得轉 tensor ，要轉回 numpy 還得 detach().cpu().numpy()[0] ，還有無處不在的 cpu(),cuda()，是不是許多執行失敗都是出在這些瑣碎小細節遺漏了呢? trident 中我重新包裝了 TrainingPlan的容器，在其中已經將訓練過程的這些細節通通都封裝好了，甚至包括多久列印出來進度、多久顯示損失函數變化曲線、多久存檔一次 ... 這些通通都可以很直覺化的設定，而且使用了Fluent Code風格，讓它可以很方便設定以及增加了可讀性與維護性。"]},{"cell_type":"code","metadata":{"id":"92IxBy6vBQNl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"1293bb77-72eb-4aac-cd2c-a385158d57e8","executionInfo":{"status":"ok","timestamp":1586193222768,"user_tz":-480,"elapsed":1528,"user":{"displayName":"尹相志","photoUrl":"","userId":"08338612072693150112"}}},"source":["model=Model(input_shape=[3,128,128],output=autoencoder)\\\n","    .with_optimizer(optimizer='Ranger',lr=2e-3,betas=(0.9, 0.999))\\\n","    .with_loss(MSELoss,loss_weight=1,name='l2 loss')\\\n","    .with_loss(EdgeLoss,loss_weight=0.2,name='edge loss')\\\n","    .with_metric(rmse,name='rmse')\\\n","    .with_regularizer('l2')\\\n","    .with_constraint('max_min_norm')\\\n","    .with_learning_rate_scheduler(reduce_lr_on_plateau,monitor='rmse',mode='min',factor=0.5,patience=5,cooldown=1,threshold=5e-4,warmup=5)\\\n","    .with_model_save_path('/content/gdrive/My Drive/DeepBelief_Course5_Examples/prewarm03_自動寶可夢編碼器/Models/pokemon_ae_tf.pth')\\\n","    .with_callbacks(TileImageCallback(batch_inteval=10,name_prefix= 'srresnet_v2_tile_image_{0}.png',include_input=True,include_output=True,include_target=False,imshow=True,\n","                                      save_path='/content/gdrive/My Drive/DeepBelief_Course5_Examples/prewarm03_自動寶可夢編碼器/results/'))\n","\n","plan=TrainingPlan()\\\n","    .add_training_item(model)\\\n","    .with_data_loader(dataset)\\\n","    .repeat_epochs(250)\\\n","    .within_minibatch_size(32)\\\n","    .print_progress_scheduling(20,unit='batch')\\\n","    .save_model_scheduling(10,unit='batch')\n"," \n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["l2 loss signature:[('output', [3, 128, 128]), ('target', [3, 128, 128])]\n","edge loss signature:[('output', [3, 128, 128]), ('target', [3, 128, 128])]\n","rmse signature:[('output', [3, 128, 128]), ('target', [3, 128, 128])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CpKvXzwjZiaP","colab_type":"text"},"source":["所以以上TrainingPlan的設定就是\n","\n","加入TrainingItem (包含了模型、優化器、損失函數、效度指標、權重正則)\n","指定 data_loader\n","重複250 epoch\n","指定大小\n","指定學習率變化模式\n","印出學習進度\n","指定模型存檔週期\n","顯示autoencoder 效果輸出圖\n","指定繪製損失函數與指標變動歷史圖的週期\n","除了預設的功能外，事實上也可以利用 Callbacks 的機制在關鍵時間點插入自定義工作，這些特性之後也會在介紹，設定完成後，只需要透過start_now()函數即可啟動。"]},{"cell_type":"code","metadata":{"id":"GISCs-yWBQNt","colab_type":"code","colab":{}},"source":["plan.start_now()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCF6K2xeZm4P","colab_type":"text"},"source":["以下是我跑了 500 epoch 的成果，是不是可以看到從雜訊到模糊，一直到越來越清晰的過程呢，只要訓練的時間夠久就能夠越來越清晰。如果您手邊沒有 gpu ，建議可以透過有免費gpu的google 的colab 來執行。"]},{"cell_type":"markdown","metadata":{"id":"HaYqy-_qZq6T","colab_type":"text"},"source":["<img src='https://docs.google.com/uc?export=download&id=1U6KEW9eanMO5wytgnMp_SZlM8xRTPGfu' width='600px'/>"]},{"cell_type":"markdown","metadata":{"id":"FwYVkVb7BQN7","colab_type":"text"},"source":["## 表徵學習 "]},{"cell_type":"markdown","metadata":{"id":"yBMd7pc-Z0Vr","colab_type":"text"},"source":["可能會有人覺得奇怪，自編碼器重建圖像就算訓練好了，到底是有甚麼作用？其實，對我們有用的並不是整個自編碼器，我們要的其實是前半段的編碼器部分。編碼器的工作是將圖片編碼成長度為 128 向量(為了避免使用到全連接層，所以實際上是 (128,1,1) 的形狀)，等於是將圖片抽出它的關鍵特徵，而這些特徵既然可以用來還原回圖像細節，這表示它必定包含了這個圖片中的關鍵訊息。這也是深度學習中表徵學習(representation learning)中最常見的手法。而這些特徵向量就可以幫助我們評估圖片中的相似性，也就是可以做到視覺搜索的效果。\n","\n","我們首先利用資料源的get_all_data()函數取出所有圖片，依序透過編碼器(autoencoder[0])產生特徵向量，並將1444 張圖片的特徵向量整併成尺寸為 (1444,128) 的向量"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"G3pQd894BQN_","colab_type":"code","colab":{}},"source":["features=[]\n","\n","\n","#dataset.data['train]\n","for img_data in dataset.get_all_data():\n","    input=to_tensor(np.expand_dims(dataset.image_transform(img_data),0))\n","    encoder_output = np.squeeze(to_numpy(autoencoder[0](input)))\n","    features.append(encoder_output)\n","\n","\n","features=np.asarray(features)\n","print(features.shape)\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sD2_ZR8Z5Je","colab_type":"text"},"source":["我們如果想要看一下特徵向量整體的效果可透過傳說中的降維神器 t-SNE ，將特徵向量降為並且視覺化。"]},{"cell_type":"code","metadata":{"id":"bCFhwPg_BQOF","colab_type":"code","colab":{}},"source":["from matplotlib import offsetbox\n","from sklearn import manifold\n","import PIL\n","from PIL import Image as Image\n","\n","def plot_embedding(X, title=None):\n","    x_min, x_max = np.min(X, 0), np.max(X, 0)\n","    X = (X - x_min) / (x_max - x_min)\n","\n","    fig =plt.figure(figsize=(18,18))\n","    ax = plt.subplot(111)\n","\n","    if hasattr(offsetbox, 'AnnotationBbox'):\n","        # 需要matplotlib 版本> 1.0才支援顯示圖片功能\n","        shown_images = np.array([[1., 1.]])  # just something big\n","        for i in range(X.shape[0]):\n","            dist = np.sum((X[i] - shown_images) ** 2, 1)\n","           \n","            shown_images = np.r_[shown_images, [X[i]]]\n","            #將向量轉圖片，且將圖片縮小至32*32\n","            img=array2image(dataset.data['train'][i]) #array2image是 trident 內的函數\n","            img = img.resize((32, 32),Image.ANTIALIAS)\n","            imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(img),X[i],pad=0, box_alignment=(0, 0))\n","            ax.add_artist(imagebox)\n","    plt.xticks([]), plt.yticks([])\n","    if title is not None:\n","        plt.title(title)\n","    display.display(fig)\n","\n","#利用 t-SNE  降維\n","tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)#利用t-sne將128特徵向量降維至2\n","X_tsne = tsne.fit_transform(features[:500,:])#為了避免圖片太密我只用了前500來處理\n","\n","plot_embedding(X_tsne, \"Embedded of Pokemon\")#繪製圖像散布圖"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbll3VTGaAW2","colab_type":"text"},"source":["看起來頗壯觀，仔細看的確位置比較接近的都有向是姿態、顏色、類型的相似，那麼我們該如何達到視覺搜索的效果呢?"]},{"cell_type":"markdown","metadata":{"id":"nAcuW7UfBQON","colab_type":"text"},"source":["## 寶可夢的相似性搜索 "]},{"cell_type":"markdown","metadata":{"id":"N-lOPbZuaEfC","colab_type":"text"},"source":["特徵向量因為在學習過程中被多次正規化，因此計算歐幾里得距離是沒有意義的，所以一般是用 cosine距離，在這種逐一比較 cosine距離的場景下，若是一個一個比實在太累了， trident 內建了一次產生各成員 cosine距離的函數element_cosine_distance，所以可以一次性的比較，然後取出距離最大者( cosine距離跟其他距離不一樣，越相似者 cosine距離越大)。\n","\n"]},{"cell_type":"code","metadata":{"id":"XqTNQj16BQOR","colab_type":"code","colab":{}},"source":["def find_similar_pokemon(idx):\n","    similarity_list=[]\n","    result=to_numpy(element_cosine_distance(features[idx:idx+1,:],features))  #element_cosine_distance逐成員計算Cosine距離\n","\n","\n","    top5=np.argsort(result)[-5:][::-1]  #找出前 5個Cosine距離最高者(Cosine距離是越高越像)\n","    similarity_list=[dataset.data['train'][idx]] #放入原圖\n","    similarity_list.append(np.ones_like(similarity_list[0])[:,:30,:]*255) # 加入白色分隔線\n","    similarity_list.extend(dataset.data['train'][top5]) #放入前 5名圖 \n","\n","    merge_img=np.concatenate(similarity_list,axis=1) #沿著寬(axis=1)疊合\n","    display.display(array2image(merge_img)) #顯示結果\n","\n","\n","\n","idx=128 #抽取一隻寶可夢\n","find_similar_pokemon(idx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TLwmB4VwaKCq","colab_type":"text"},"source":["我們當然也可以把傳入的索引值透過隨機的方式來指派，這樣我們就可以觀察到更多樣的搜索結果"]},{"cell_type":"code","metadata":{"id":"MCbYsGiNBQOZ","colab_type":"code","colab":{}},"source":["import random\n","find_similar_pokemon(random.choice(range(1444)))"],"execution_count":0,"outputs":[]}]}